<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习入门系列8: Tips For DeepLearning_2 全程高能!</title>
      <link href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%978-Tips-For-DeepLearning-2-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/"/>
      <url>/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%978-Tips-For-DeepLearning-2-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>前一篇中文讨论到，在深度学习过程中如果在训练集效果差怎么办，这里接着讨论后半部分，在训练集得到了想要了的效果，但是测试集（<strong>验证集，或者是有标签的一些数据</strong>）效果并不理想应该怎么办？有如下几种方案、方法可以参考一下：</p><ul><li>Early Stopping</li><li>Regularization</li><li>Dropout</li></ul><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><a href="https://user-images.githubusercontent.com/60562661/74507861-997b9f00-4f38-11ea-91fa-f16cb7862228.png" data-fancybox="group" data-caption="1581651442213" class="fancybox"><img alt="1581651442213" style="zoom: 80%;" title="1581651442213" data-src="https://user-images.githubusercontent.com/60562661/74507861-997b9f00-4f38-11ea-91fa-f16cb7862228.png" class="lazyload"></a><p>这个方法大致提一下。深度学习中我们有一个假设：训练集和测试集分布是一样的。但是实际上可能并不会如此，我们训练模型应该在验证机loss最低的时候停下来，这就是这个方法的基本思想，具体的可以查阅下 文档。这里我自己主要也是了解一下即可，知道有这么一回事。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ol><li>正则化的目的是使得函数更加的平滑，因此正则化一般不对 bias 偏置做；</li><li>正则化会使得参数变小</li><li>正则化并不是非常的重要，效果不会非常显著</li></ol><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><a href="https://user-images.githubusercontent.com/60562661/74507869-9d0f2600-4f38-11ea-9383-46421037d9da.png" data-fancybox="group" data-caption="1581651915129" class="fancybox"><img alt="1581651915129" style="zoom: 67%;" title="1581651915129" data-src="https://user-images.githubusercontent.com/60562661/74507869-9d0f2600-4f38-11ea-9383-46421037d9da.png" class="lazyload"></a><p>首先是右上角L2范数，是w的平方和。</p><p>然后是下面的公式推导，L‘是损失函数，L’对某一个w微分，就是后面的结果，然后更新参数公式也很顺理成章，整理到最后就是会在w之前✖一个系数，并且这个系数通常是一个很小的值，整个这个系数接近1，因此每个参数每次更新前会越来越接近0；第一项与第二项最后会实现平衡。</p><p><strong>这个手段也成为 Weight Decay，权重衰减</strong></p><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h3><p>L1范数是绝对值求和。那么绝对值微分问题就是在真的走到0时直接随便丢一个value比如0当作微分。</p><p><a href="https://user-images.githubusercontent.com/60562661/74507871-9ed8e980-4f38-11ea-9820-1248212f60f4.png" data-fancybox="group" data-caption="1581652966101" class="fancybox"><img alt="1581652966101" title="1581652966101" data-src="https://user-images.githubusercontent.com/60562661/74507871-9ed8e980-4f38-11ea-9820-1248212f60f4.png" class="lazyload"></a></p><p>L‘对于某一个w的微分就是上图中所推导的，后面的sgn(w)意为：w为正则为1，w为负则为-1.</p><p>更新参数时总是在后面减去一项学习率 * 权重 * （1或-1），w为正数时减去一个数，w为负数时加上一个数，总之就是使得参数更加地接近0</p><h3 id="Contrast"><a href="#Contrast" class="headerlink" title="Contrast"></a>Contrast</h3><p>L1、L2正则化都是使得参数变小，但是略有不同；</p><ul><li>L1每次剪掉固定的值，</li><li>L2则是每次乘以一个固定的值，</li></ul><p>如果有一个参数很大，那么用L2正则化更新就比较快；用L1正则化更新依然很慢；</p><p>如果有一个参数很小，那么L2正则化更新就很慢；L1正则化 更新会比较快</p><p><strong>正则化中的权值衰减，与人的神经网络有异曲同工之妙</strong></p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p><a href="https://user-images.githubusercontent.com/60562661/74508193-5f5ecd00-4f39-11ea-9ce8-7ec0eb6d8384.png" data-fancybox="group" data-caption="1581653771487" class="fancybox"><img alt="1581653771487" title="1581653771487" data-src="https://user-images.githubusercontent.com/60562661/74508193-5f5ecd00-4f39-11ea-9ce8-7ec0eb6d8384.png" class="lazyload"></a></p><p>Dropout方法训练流程是这样：</p><ul><li><p>设置一个 dropout rate ：p%,也就是说在训练时每层会有 p% 的神经元被丢掉；</p></li><li><p>然后每次更新参数都会重新采样丢掉的神经元。</p></li></ul><p>Dropout的测试：</p><p>测试期间所有的神经元保留，然后最后丢失率是多少，每个w 都要乘上（1 - 丢失率）。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>dropout可以解释为训练了一系列的神经网络，然后最后输出的值取了个平均</p><a href="https://user-images.githubusercontent.com/60562661/74507874-9f718000-4f38-11ea-8097-eedc538b23e9.png" data-fancybox="group" data-caption="1581661977792" class="fancybox"><img alt="1581661977792" style="zoom: 50%;" title="1581661977792" data-src="https://user-images.githubusercontent.com/60562661/74507874-9f718000-4f38-11ea-8097-eedc538b23e9.png" class="lazyload"></a><p>如上图，dropout在所有的weight✖ (1-p%) 就取得了数个神经网络做平均的相近的结果。这也就是这个方法最神奇的地方。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在深度学习中遇到的问题分为 <code>training data set</code>  <code>test set data</code> 效果差</p><h3 id="在训练集效果差"><a href="#在训练集效果差" class="headerlink" title="在训练集效果差"></a>在训练集效果差</h3><h4 id="梯度弥散"><a href="#梯度弥散" class="headerlink" title="梯度弥散"></a>梯度弥散</h4><ul><li>relu 激活函数</li><li>maxout 激活函数</li></ul><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul><li>Adagrad</li><li>RMSProp</li></ul><h4 id="局部最小化"><a href="#局部最小化" class="headerlink" title="局部最小化"></a>局部最小化</h4><ul><li>Momentum 算法</li><li>Adam </li></ul><h3 id="在训练集效果好但是在测试集效果差-过拟合"><a href="#在训练集效果好但是在测试集效果差-过拟合" class="headerlink" title="在训练集效果好但是在测试集效果差(过拟合)"></a>在训练集效果好但是在测试集效果差(过拟合)</h3><ul><li>Easy Stopping</li><li>L1、L2正则化</li><li>Dropout</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列7: Tips For DeepLearning_1 全程高能!</title>
      <link href="/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%977-Tips-For-DeepLearning-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/"/>
      <url>/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%977-Tips-For-DeepLearning-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>本文主要讲深度学习中常见的一些但是又理解不透彻的方法原理以及用途，全程高能！</p><p>主要有：</p><ul><li>深度学习流程</li><li>Activation Function（各种）</li><li>各种梯度下降法</li><li>L1、L2 正则化</li><li>dropout</li></ul><h2 id="DeepLearning-流程"><a href="#DeepLearning-流程" class="headerlink" title="DeepLearning 流程"></a>DeepLearning 流程</h2><a href="https://user-images.githubusercontent.com/60562661/74453826-ed956d80-4ebd-11ea-8ce3-89ece01340d1.png" data-fancybox="group" data-caption="1581604363019" class="fancybox"><img alt="1581604363019" style="zoom:50%;" title="1581604363019" data-src="https://user-images.githubusercontent.com/60562661/74453826-ed956d80-4ebd-11ea-8ce3-89ece01340d1.png" class="lazyload"></a><ol><li>首先是之前文章提到过的深度学习的三个步骤，这三个步骤会搭建起一个神经网络；</li><li>训练神经网络，检测网络对于训练集的效果,此时如果效果很差就要回去调整模型，继续训练，知道得到比较好的结果</li><li>此时在训练集效果达到了，开始在测试集(<strong>验证集</strong>)进行测试，如果效果不好就说明过拟合了，需要采取一些措施。</li></ol><p><strong>这里要注意，不是说效果一差就是过拟合，要理性分析。</strong></p><h2 id="Training-set-效果差的原因及其解决"><a href="#Training-set-效果差的原因及其解决" class="headerlink" title="Training set 效果差的原因及其解决"></a>Training set 效果差的原因及其解决</h2><h3 id="Vanishing-Gradient"><a href="#Vanishing-Gradient" class="headerlink" title="Vanishing Gradient"></a>Vanishing Gradient</h3><p>首先思考一个问题，为什么需要激活函数？ </p><p>一个普通的全连接网络，如果没有激活函数，那么所有的操作都是乘积、求和，这就是一个线性的网络，表达能力非常差，也不是我们所需要的，因此传统的在每个层后面输出时会加个sigmoid这样的非线性函数。也就是说，激活函数可以使我们的网络编程非线性的。</p><h4 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h4><a href="https://user-images.githubusercontent.com/60562661/74453819-ea9a7d00-4ebd-11ea-9848-b5bbf6ac7e49.jpg" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74453819-ea9a7d00-4ebd-11ea-9848-b5bbf6ac7e49.jpg" class="lazyload"></a><p>如图所示，所有的输入经过sigmoid会强行压缩到0-1之间，这样会使函数变成非线性的，但是也随之带来了问题：强行将所有之压缩到0-1，随着网络层数的加深，反向传播时靠近输出层的梯度值还比较大，但是输入层附近的层梯度都会很小，梯度从后往前越来越小直到消失，这就是<strong>梯度弥散</strong> (<strong>gradient Vanish</strong> )问题.</p><ul><li>靠近输出层的参数更新幅度比较快，但是此时靠近输入层的梯度已经很小接近于0，参数基本无更新，也就是还是随机值的状态来更新后面的参数，所以此时后面更新的参数问题就很大！</li><li>当输入层某个w发生变化就算变化非常大，而sigmoid的值变化幅度确非常的小，因为函数曲线很缓，如下图</li></ul><a href="https://user-images.githubusercontent.com/60562661/74453863-f8e89900-4ebd-11ea-9a69-9dbe824afeab.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 67%;" data-src="https://user-images.githubusercontent.com/60562661/74453863-f8e89900-4ebd-11ea-9a69-9dbe824afeab.png" class="lazyload"></a><h4 id="ReLu-函数"><a href="#ReLu-函数" class="headerlink" title="ReLu 函数"></a>ReLu 函数</h4><a href="https://user-images.githubusercontent.com/60562661/74453834-eff7c780-4ebd-11ea-92b4-a3e606985972.png" data-fancybox="group" data-caption="1581605904109" class="fancybox"><img alt="1581605904109" style="zoom: 50%;" title="1581605904109" data-src="https://user-images.githubusercontent.com/60562661/74453834-eff7c780-4ebd-11ea-92b4-a3e606985972.png" class="lazyload"></a><p>如图所示，输出值a小于0，则 <code>Relu(a) = 0</code>  否则，<code>Relu(a) = a</code></p><p>这个激活函数的出现解决了梯度弥散问题</p><a href="https://user-images.githubusercontent.com/60562661/74453841-f2f2b800-4ebd-11ea-9560-0a34ec5c413e.png" data-fancybox="group" data-caption="1581606094265" class="fancybox"><img alt="1581606094265" style="zoom:50%;" title="1581606094265" data-src="https://user-images.githubusercontent.com/60562661/74453841-f2f2b800-4ebd-11ea-9560-0a34ec5c413e.png" class="lazyload"></a><p>解决方式如图，值为0的神经元可以去掉，整个网络就是一个线性函数了。</p><p>这里有一个问题是：0的神经元消除，整个网络就是一个线性函数，这不符合我们要求啊？</p><p>可以这样解释，改变了数据输入，神经元的连线发生变化，整个网络依然是非线性的。</p><p><strong>着重理解Relu，它的一系列的变体函数就不说了，比较相似</strong></p><p>CNN文章提到的<code>Max Pooling</code> 不可导，和这个就很相似了，每次都是取最大值，然后其他的神经元都丢掉，整个网络又是一个细长的线性神经网络。<strong>其实是和下面的 Max-out函数比较相似</strong></p><h4 id="Max-Out-函数"><a href="#Max-Out-函数" class="headerlink" title="Max-Out 函数"></a>Max-Out 函数</h4><p><a href="https://user-images.githubusercontent.com/60562661/74454478-ede23880-4ebe-11ea-820a-d34dbd2c3cf6.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74454478-ede23880-4ebe-11ea-820a-d34dbd2c3cf6.png" class="lazyload"></a></p><p>这个方法是打组思想，Relu就是它的一个个例，这里就不细说了，relu用的多一点。</p><p><strong>有个问题：这个每次只取最大的这样有的神经元就不会被train到。因为每次数据不同得到的最大值也会不同，所以所有的weight都会得到更新，都会被train到。</strong></p><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>梯度下降法的时候，会有局学习率问题，所以出现了一些梯度下降法的变体。</p><p><a href="https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/#%E6%A6%82%E8%BF%B0%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B" target="_blank" rel="noopener">Gradient Descent</a>、<a href="https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96" target="_blank" rel="noopener">Adagrad Gradient Descent</a> 、以及 <a href="[https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96](https://huaqi.blue/2020/02/10/深度学习入门系列一-梯度下降法-②/#梯度下降法及其优化)">SGD</a> 在之前文章都有讲过,不清楚可以返回看一下。</p><p>Adagrad梯度下降法实现了自适应的学习率，但是实际上我们面对的比这个所能解决的问题更加复杂，如下图。</p><p><a href="https://user-images.githubusercontent.com/60562661/74453867-fab25c80-4ebd-11ea-9131-f4a5cc825046.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74453867-fab25c80-4ebd-11ea-9131-f4a5cc825046.jpg" class="lazyload"></a></p><ul><li>左边的在w1方向，从左到右是比较平坦的，所以一个学习率就可以；</li><li>右边的图同样在w1方向比较缓的地方学习率就应该小点儿，突然陡峭学习率又应该大一点。<strong>因此需要更加动态的调整学习率。</strong></li></ul><h4 id="Root-Mean-Square-Prop-RMSProp"><a href="#Root-Mean-Square-Prop-RMSProp" class="headerlink" title="Root Mean Square Prop(RMSProp)"></a>Root Mean Square Prop(RMSProp)</h4><a href="https://user-images.githubusercontent.com/60562661/74453848-f5551200-4ebd-11ea-8508-f858952d1793.png" data-fancybox="group" data-caption="1581608584478" class="fancybox"><img alt="1581608584478" style="zoom: 67%;" title="1581608584478" data-src="https://user-images.githubusercontent.com/60562661/74453848-f5551200-4ebd-11ea-8508-f858952d1793.png" class="lazyload"></a><p>与之前的Adagrad方法非常类似，不同的是，这个方法可以控制权重，更偏向于过去的信息还是新的梯度信息，可以通过设置权重值来控制</p><h3 id="Local-Minimum"><a href="#Local-Minimum" class="headerlink" title="Local  Minimum"></a>Local  Minimum</h3><h4 id="Momentum-冲量、惯性-算法"><a href="#Momentum-冲量、惯性-算法" class="headerlink" title="Momentum (冲量、惯性)算法"></a>Momentum (冲量、惯性)算法</h4><p>如下图，在求出当前阶段的梯度值后，不只是考虑当前的梯度方向，同时考虑了前一个梯度方向，然后做个加权和得到新的方向。</p><a href="https://user-images.githubusercontent.com/60562661/74453853-f6863f00-4ebd-11ea-959d-a1dc06789b2e.png" data-fancybox="group" data-caption="1581609144187" class="fancybox"><img alt="1581609144187" style="zoom: 40%;" title="1581609144187" data-src="https://user-images.githubusercontent.com/60562661/74453853-f6863f00-4ebd-11ea-959d-a1dc06789b2e.png" class="lazyload"></a><p>这可以说是根据现实生活中的惯性，会继续向前走一点。</p><h4 id="Adam-优化算法"><a href="#Adam-优化算法" class="headerlink" title="Adam 优化算法"></a>Adam 优化算法</h4><p>Adam则是 RMSProp、Momentum两个梯度下降算法的集大成者。在深度学习框架中是已经封装好的。这个并未深究，我觉得理解了基础的就可以。</p><h2 id="关于测试集效果差，且听下回分解。"><a href="#关于测试集效果差，且听下回分解。" class="headerlink" title="关于测试集效果差，且听下回分解。"></a>关于测试集效果差，且听下回分解。</h2><p><a href="https://user-images.githubusercontent.com/60562661/74454502-f5094680-4ebe-11ea-9a65-ca37f68a6660.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74454502-f5094680-4ebe-11ea-9a65-ca37f68a6660.png" class="lazyload"></a></p><p>主要讲正则化、Dropout这两块。上图中前两种方法都是比较经典的，不只是深度学习的，而正则化用的多一点，所以主要理解一下正则化； Dropout则是非常具有深度学习风格，所以需要深究。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列6: Convolution Neural Network(CNN)卷积神经网络</title>
      <link href="/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%976-Convolution-Neural-Network-CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%976-Convolution-Neural-Network-CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h2 id="CNN的提出"><a href="#CNN的提出" class="headerlink" title="CNN的提出"></a>CNN的提出</h2><p><strong>其一，</strong>之前所提到的线性回归、比较简单逻辑回归都是全连接层<strong>(Full-Connected)</strong> ,那么在图像处理领域输入数据都是图像，现实中一张很小的图 <strong>100 * 100</strong> ，分辨率已经很低了，然是依然有<strong>30000 维数据</strong>，(这里默认图象是彩色图三通道的)，然后后面再堆几层网络，参数量实在是巨大，这是全连接网络的缺陷；</p><p><strong>其二，</strong>基于现实的观察有以下基点：</p><ul><li><p>假设CNN中每一个神经元都是用来识别某一个<strong>pattern</strong>[例如：鼻子，嘴，手臂] (实际上大概也是这样工作的)</p></li><li><p>人们在辨识一些小的部分比如鸟喙时，并不需要遍历一张图的所有信息，而是看到图片的一小部分就可以捕捉到需要的信息；</p></li><li><p>同一个部分(鸟喙)在图像中可能会出现在不同的位置，因此CNN的神经元以相同的参数就可以发现不同位置的鸟喙而不用重新学习参数</p></li><li><p>图像进行下采样，并不会影响我们对图片的观察(不包括比较极端的)；而图像较小的时候像素比较少此时也会减少参数</p></li></ul><p>基于以上，CNN卷积神经网络就正式提出了，并且在计算机视觉领域(影像处理)非常有效，几乎所有的任务，第一步都是要用卷积神经网络来提取特征。</p><h2 id="CNN一般架构"><a href="#CNN一般架构" class="headerlink" title="CNN一般架构"></a>CNN一般架构</h2><p>卷积神经网络一般是输入图像，然后经过 （卷积层、池化层）这两个一直重复，然后输出的像素拉平(flatten操作将当前值转变为一维向量)，连接上全连结网络输出，如下图：</p><a href="https://user-images.githubusercontent.com/60562661/74443982-1e21db00-4eaf-11ea-973c-7b46e241d959.png" data-fancybox="group" data-caption="1581600351984" class="fancybox"><img alt="1581600351984" style="zoom: 67%;" title="1581600351984" data-src="https://user-images.githubusercontent.com/60562661/74443982-1e21db00-4eaf-11ea-973c-7b46e241d959.png" class="lazyload"></a><h2 id="Convolution计算流程"><a href="#Convolution计算流程" class="headerlink" title="Convolution计算流程"></a>Convolution计算流程</h2><p><strong>首先，CNN中要训练的参数就是卷积核的每个像素的数值</strong></p><h3 id="单通道卷积计算"><a href="#单通道卷积计算" class="headerlink" title="单通道卷积计算"></a>单通道卷积计算</h3><a href="https://user-images.githubusercontent.com/60562661/74443985-1f530800-4eaf-11ea-8059-661371e1f691.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 67%;" data-src="https://user-images.githubusercontent.com/60562661/74443985-1f530800-4eaf-11ea-8059-661371e1f691.png" class="lazyload"></a><p>如上图，用filter1在图像6<em>6图像上滑动，从左上角开始，步长为1，在每个窗格对应位置相乘然后加起来输出一个新的值，此时就会形成一个新的4 *</em> 4的 img ，称为特征图*<em>Feature Map *</em>。</p><p>此时有一个卷积核，就输出一张特征图，两个卷积核就输出两张特征图，以此类推。</p><h3 id="多通道卷积计算"><a href="#多通道卷积计算" class="headerlink" title="多通道卷积计算"></a>多通道卷积计算</h3><a href="https://user-images.githubusercontent.com/60562661/74443978-1c581780-4eaf-11ea-923b-a7975171f4d7.png" data-fancybox="group" data-caption="1581598043317" class="fancybox"><img alt="1581598043317" style="zoom: 67%;" title="1581598043317" data-src="https://user-images.githubusercontent.com/60562661/74443978-1c581780-4eaf-11ea-923b-a7975171f4d7.png" class="lazyload"></a><p>如果输入的图像是三通道的，那么每个卷积核对应的也是三通道的，注意此时计算可能是：</p><p>卷积核的第一个通道与图像红色通道进行卷积运算，卷积核的第二个通道与图像绿色通道进行卷积运算，卷积核的第三个通道与图像蓝色通道进行卷积运算，然后 卷积核三个通道输出的img对应位置相加，形成一个新的1个通道的img，就是这个卷积核所输出的 <strong>Feature Map</strong>  。这里注意的是： 对于多通道图像，<code>一个卷积核进行卷积运算后所输出的依然是一个 feature map，而不是9个(3*3).</code></p><h2 id="Convolution-amp-Neural-Network"><a href="#Convolution-amp-Neural-Network" class="headerlink" title="Convolution & Neural Network"></a>Convolution & Neural Network</h2><p>以上讲了卷积的运算方式，那么卷积与神经网络，与全连接网络有什么关系呢？</p><p><strong>卷积实际上就是全连接网络(去掉一些weight) !</strong></p><a href="https://user-images.githubusercontent.com/60562661/74443980-1d894480-4eaf-11ea-81f5-dde23c1db352.png" data-fancybox="group" data-caption="1581598946132" class="fancybox"><img alt="1581598946132" style="zoom: 67%;" title="1581598946132" data-src="https://user-images.githubusercontent.com/60562661/74443980-1d894480-4eaf-11ea-81f5-dde23c1db352.png" class="lazyload"></a><p>分析一下这张图，</p><ul><li><p>首先右边蓝色的 1 2 3 4 ···一直到16，表示的是将左边6*6的图像拉平（这里没有画完），蓝色的框里的数字是每个像素的值；</p></li><li><p>然后上面是个3*3的卷积核，每个像素用不同颜色的⚪圈了起来；</p></li><li><p>然后上图右边部分橙色的 3，-1 就是 卷积核与图像滑动过的区域做的卷积计算得到的数值，将卷积核卷积后的4*4的img也拉平，就得到了右边的 3 ，-1  （这里用3和-1举例子所以没有画完）</p></li></ul><h3 id="大量参数的减少"><a href="#大量参数的减少" class="headerlink" title="大量参数的减少"></a>大量参数的减少</h3><p>卷积之后得到的图像的每个像素也就是右边的3，-1 等，可以看作是一个神经元，其中 卷积核做图像左上角的时候，计算刚好是与原来的6<em>6图像的 编号为 1 2 3 7 8 9 13 14 15 的像素进行的，因此“*</em>3**”这个神经元就连接到了编号为 1 2 3 7 8 9 13 14 15 的像素，-1是同样的道理。这时候，如果计算参数量，就是 16 * 9 = 144 个参数，而此时如果用全连接层的话，就是 36 * 16 = 576 个参数，已经少了很多了</p><h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>上图中右边部分的神经元，并不是说所有的参数都要计算。一个卷积核中同一个像素滑动过的值他们之间的权重都是强迫相等的。举个例子，卷积核中的第一个像素(深红色圆圈)，与6<em>6的图像在左上角计算卷积时对应的编号为1的像素，卷积核向右滑动一次后，该像素(深红色圆圈)对应的是编号为2的像素，因此 上图右边部分 1 号像素和 右边的神经元3 ， 2号像素与右边的神经元-1之间连接都用的是深红色，这两条线的参数就是相等的。所以同理，上图右边部分连线中颜色相同的权值都是共享的。(*</em>Share Weights**) 此来再来计算一下参数量，就只有9个了。</p><p>这其实也不难理解，一开始文章就提到，卷积神经网络的参数就是卷积核的像素值，这里是3*3的卷积核9个像素，所以也就只有9个参数了。到这里已经是全连接网络的 1/64 了，也就是减少了64倍的参数，这在 上百万参数是减少的就更明显了！</p><p><strong>到这里，已经理解了卷积神经网络的计算方式以及如何减少参数</strong></p><h2 id="池化-Max-pooling"><a href="#池化-Max-pooling" class="headerlink" title="池化 Max pooling"></a>池化 Max pooling</h2><p><a href="https://user-images.githubusercontent.com/60562661/74443974-1a8e5400-4eaf-11ea-91d0-250625a1d4ad.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74443974-1a8e5400-4eaf-11ea-91d0-250625a1d4ad.jpg" class="lazyload"></a></p><p>在卷积输出的特征图基础上，以2*2为单位，每个红色框里选出最的值组成一个新的img，这就是最大池化；</p><p>平均池化就是一个红色框里所有的像素值取平均。 </p><p>经过池化，图像尺寸变为 2*2 </p><p><strong>池化层采用最大池化方式，那么怎么求微分呢？不可导就不能梯度下降，这个下一篇文章会说。</strong></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>以上就是CNN，卷积神经网络，工作方式可以理解为某一层的神经元识别一个 pattern ，然后全连接层组合这些个 pattern 最后提取出高质量的特征。 这个可以自己求证一下。大概可以这么解释。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列5: 反向传播BP算法</title>
      <link href="/2020/02/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%974-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADBP%E7%AE%97%E6%B3%95/"/>
      <url>/2020/02/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%974-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADBP%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>之前讲过机器学习的三个步骤，<strong>深度学习</strong><code>Deep Learning</code>非常的类似，可以概括为以下几步：</p><ul><li>(设置函数) -> 搭建<strong>神经网络</strong></li><li>(函数的好坏定义) -> 设置<strong>损失函数</strong></li><li>(找出最优函数) -> <strong>反向传播</strong>更新参数</li></ul><p>第一步之前的设置函数，在这里用神经网络来替代了；</p><p>在线性回归逻辑回归中可以直接计算梯度，但是深度学习神经网络比较深，不能一下子求出梯度，因此本文主要来探讨一下反向传播 <code>Back Propagation</code>算法。</p><p><strong>同时，本文会附上手动搭建神经网络、计算梯度、实现反向传播的代码，纯手写只用到了numpy库！</strong></p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><p>神经网络的反向传播并不需要很高深的数学知识，需要掌握<strong>链式求导法则 (Chain Rule)</strong>。下面会一步步从数学上求出微分 ，并且理解这种算法的精妙之处。</p><h2 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h2><p><a href="https://user-images.githubusercontent.com/60562661/74358378-50263500-4dfc-11ea-8f03-2fac002d131d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74358378-50263500-4dfc-11ea-8f03-2fac002d131d.png" class="lazyload"></a></p><p>一个有代表性的例子，输入层经过神经网络得到输出值，与真实值之间存在误差，这里用交叉熵作为损失函数，因此在这里，要求梯度也就是求损失函数对于w的偏微分。</p><h2 id="层层深入"><a href="#层层深入" class="headerlink" title="层层深入"></a>层层深入</h2><p><a href="https://user-images.githubusercontent.com/60562661/74358360-4b618100-4dfc-11ea-8411-9478cb1f4a0f.png" data-fancybox="group" data-caption="1581522297605" class="fancybox"><img alt="1581522297605" title="1581522297605" data-src="https://user-images.githubusercontent.com/60562661/74358360-4b618100-4dfc-11ea-8411-9478cb1f4a0f.png" class="lazyload"></a></p><p>把上面具体的神经网络展开，假设只有两个神经元，这里只对w求微分，b的方式是一样的，所以以w为例。</p><p>首先，根据上图的函数以及chain rules，损失函数C对w的偏微分可以拆解为两部分</p><ul><li>C 对于 z 的偏导数 </li><li>z 对于 w 的偏导数</li></ul><h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>（先讲第二部分）其中，z 对于 w 的偏导数比较容易，可以很容易的看出来 z 对于 w1 的偏导数就是w之前的输入值，也就是 x1，同理 z 对于 w2 的偏导数就是 x2，下面的图强化一下 z 对于 w 的偏导数</p><a href="https://user-images.githubusercontent.com/60562661/74358383-51eff880-4dfc-11ea-9f7b-87aea43f3483.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74358383-51eff880-4dfc-11ea-9f7b-87aea43f3483.png" class="lazyload"></a><p>可以观察到z对于每个w的偏导数就是 当前权重所之前的输入值，这样比较拗口，英语会比较好理解：</p><p><strong>The value of the input connected by the weight</strong> .通俗的说就是这条线从哪里出来，出来的那个节点值就是z对于这条线也就是这个w的偏微分。</p><p>这里也可以看出，前向传播可以算出每个中间值，也就是计算出了每个梯度的上述第二部分。</p><h3 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>下面看第一部分比较复杂的，也就是 C 对于 z 的偏导数。</p><p><a href="https://user-images.githubusercontent.com/60562661/74358367-4dc3db00-4dfc-11ea-81b3-7bf35d6c3b66.png" data-fancybox="group" data-caption="1581523046931" class="fancybox"><img alt="1581523046931" title="1581523046931" data-src="https://user-images.githubusercontent.com/60562661/74358367-4dc3db00-4dfc-11ea-81b3-7bf35d6c3b66.png" class="lazyload"></a></p><p>上图中 z 经过 sigmoid 函数 得到 a，a 继续传播到下一层，此时 C 对于 z 的偏导数可以转化为上图中的下面公式所写的。在求和的两部分中，同样的各自又都分为两部分，与上述的两部分类似。z’ 对于 a 的偏导数很容易，就直接是 w3 ，相应的 z’‘ 就是 w4.</p><p>所以此时问题就转化为 C对于z’ C对于z‘’ 的偏微分，如果这两部分知道那么就可以求出来 C对a的偏微分，同样的C对z的偏微分也就求出来了，也就解决了这部分问题。以下内容是<strong>关键：</strong></p><h3 id="反求"><a href="#反求" class="headerlink" title="反求"></a>反求</h3><a href="https://user-images.githubusercontent.com/60562661/74358708-e195a700-4dfc-11ea-978f-34ace433b4ce.png" data-fancybox="group" data-caption="1581523822435" class="fancybox"><img alt="1581523822435" style="zoom:50%;" title="1581523822435" data-src="https://user-images.githubusercontent.com/60562661/74358708-e195a700-4dfc-11ea-978f-34ace433b4ce.png" class="lazyload"></a><p>上图的下面的公式只是把值带入了前一步中的公式，但是可以想象一下，这里面的 乘法、加法 操作 很像是神经网络的前向传播，所以这里就可以想象成一个新的神经网络，只不过是反过来计算的，这时候就会计算出来 C 对于 z 的偏微分。值得注意的是 上图中 <code>sigmoid’(z)</code> 是个常数，因为z前向传播时已经计算出来了，所以这里就是计算一个数而已。因此现在到这一步，<strong>说明知道后面两项的偏微分可以求出前面的。</strong> 此时问题依然是 C对于z’ C对于z‘’ 的偏微分。</p><h3 id="大胆假设、细心求证"><a href="#大胆假设、细心求证" class="headerlink" title="大胆假设、细心求证"></a>大胆假设、细心求证</h3><h4 id="假设一-easy"><a href="#假设一-easy" class="headerlink" title="假设一 (easy)"></a>假设一 (easy)</h4><p>假设 <strong>z‘ z’‘</strong> 之后经过激活函数直接是最终的输出，此时求微分就很简单了，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74358371-4ef50800-4dfc-11ea-851d-3f8bb542090d.png" data-fancybox="group" data-caption="1581524647050" class="fancybox"><img alt="1581524647050" title="1581524647050" data-src="https://user-images.githubusercontent.com/60562661/74358371-4ef50800-4dfc-11ea-851d-3f8bb542090d.png" class="lazyload"></a></p><p>其中，C 对于 y1 的偏微分就是损失函数的偏微分，y1 对于 z’ 的偏微分就是根据<strong>激活函数</strong>（上图最后的橙色圆圈）求出微分很容易，z‘’ 同理。</p><p>在此种假设下，此时已经得出了 C 对于 z‘、 C 对于 z’‘ 的偏微分，回溯到前一个步骤，就求出了 C对于z的偏导，在往前回到最初步，发现此时已经求出了两个 需要的条件，此时就可以算出 C 对于 w1 ，C  对于 w2 的偏微分。</p><p><strong>也就是说，忙活到现在，也就只是算出来了第一个神经元的两条线(2个w)的梯度！</strong></p><h4 id="假设二-Normal"><a href="#假设二-Normal" class="headerlink" title="假设二 (Normal)"></a>假设二 (Normal)</h4><p>假设 <strong>z‘ z’‘</strong> 之后 依然有很多层，如下图：</p><p> <a href="https://user-images.githubusercontent.com/60562661/74358374-4f8d9e80-4dfc-11ea-9285-7f803a3a4c5f.png" data-fancybox="group" data-caption="1581525161904" class="fancybox"><img alt="1581525161904" title="1581525161904" data-src="https://user-images.githubusercontent.com/60562661/74358374-4f8d9e80-4dfc-11ea-9285-7f803a3a4c5f.png" class="lazyload"></a></p><p>由<code>反求</code>部分我们已经知道想要求 C对于 z’（or z‘’） 的偏导数，需要知道后面 C 对于 Za 及 Zb 的偏导数，所以需求会一直往后寻找，递归这个过程，知道到达输出层，然后一层层往前就会求出来最初始的梯度，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74358380-51576200-4dfc-11ea-9ec1-958f51f62321.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74358380-51576200-4dfc-11ea-9ec1-958f51f62321.png" class="lazyload"></a></p><p>至此，如果理解了这些，就已经理解了反向传播的原理了。</p><h2 id="Conclusion-amp-Question"><a href="#Conclusion-amp-Question" class="headerlink" title="Conclusion & Question"></a>Conclusion & Question</h2><h3 id="BP算法总结"><a href="#BP算法总结" class="headerlink" title="BP算法总结"></a>BP算法总结</h3><p><code>Back Propagation</code>  算法分为两部分</p><ul><li>前向传播 求出 z 对于 w 的偏导数</li><li>反向传播 求出 C 对于 z 的偏导数 </li><li>两个值相乘就是梯度</li></ul><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>从前到后的传播直接计算每个参数的梯度为什么比BP算法差?</p><h4 id="前向传播计算梯度"><a href="#前向传播计算梯度" class="headerlink" title="前向传播计算梯度"></a>前向传播计算梯度</h4><p>从前到后直接传播计算梯度，第一层的w需要知道后面所有的层的梯度，此时会进行一趟计算；2-end；</p><p>继续求第二层w 的梯度，需要知道后面所有层的梯度，也就是 3 - end； 最后加起来就是：</p><p>end - 2 + end - 3 + end - 4 + ….. + end-0,z明显计算量不小！</p><h4 id="反向传播计算梯度"><a href="#反向传播计算梯度" class="headerlink" title="反向传播计算梯度"></a>反向传播计算梯度</h4><p>从最后一层开始计算，先计算出最后一层梯度，可以直接计算出来，这样每次往前计算不用再一直累加，因此计算量小很多。所以说BP算法刚好就是利用了原来的网络和参数而且可以用和前向传播相同的计算量计算出所有w的梯度。这就是BP算法的精妙之处！</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在这里手动搭建了一个神经网络，暂时没有考虑b，因为只是用来加深理解，又一个输入层，两个隐藏层，一个输出层，每层四个神经元。所有参数都是手动计算梯度。</p><p>根据以上分析的反向传播算法可以总结出以下几步：</p><ul><li>前向传播一遍计算出所有节点的值</li><li>反向传播一遍计算出所有结点的偏微分</li><li>做乘法求出所有的梯度进行更新</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#Generate data</span></span><br><span class="line"><span class="comment"># Forward Node</span></span><br><span class="line">x_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">y_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">z_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">p_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#weight</span></span><br><span class="line">x_y_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y_z_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">z_p_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">#backward node</span></span><br><span class="line">x_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">y_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">z_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">p_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#TARGET</span></span><br><span class="line">target = np.array([<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.3</span>,<span class="number">0.1</span>])</span><br><span class="line"><span class="comment">#loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SquareErrorLoss</span><span class="params">(output, target)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(output)):</span><br><span class="line">        loss = loss + (output[i] - target[i])**<span class="number">2</span></span><br><span class="line">    loss = loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graident</span></span><br><span class="line">lr = <span class="number">0.0000001</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">500000</span>):</span><br><span class="line">    <span class="comment"># forward0</span></span><br><span class="line">    y_F = np.matmul(x_F, x_y_w)</span><br><span class="line">    z_F = np.matmul(y_F, y_z_w)</span><br><span class="line">    p_F = np.matmul(z_F, z_p_w)  <span class="comment"># 得到输出</span></span><br><span class="line">    loss_end = SquareErrorLoss(p_F, target)</span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    p_B = <span class="number">2</span>*p_F  <span class="comment"># end grad</span></span><br><span class="line">    z_B = np.matmul(p_B, z_p_w.T)</span><br><span class="line">    y_B = np.matmul(z_B, y_z_w.T)</span><br><span class="line">    <span class="comment"># print(z_F[0])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad</span></span><br><span class="line">    z_p_w_grad = [np.dot(z_F[<span class="number">0</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">1</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">2</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">3</span>], p_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    y_z_w_grad = [np.dot(y_F[<span class="number">0</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">1</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">2</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">3</span>], z_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    x_y_w_grad = [np.dot(x_F[<span class="number">0</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">1</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">2</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">3</span>], y_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    x_y_w = x_y_w - lr * np.array(x_y_w_grad)</span><br><span class="line">    y_z_w = y_z_w - lr * np.array(y_z_w_grad)</span><br><span class="line">    z_p_w = z_p_w - lr * np.array(z_p_w_grad)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"当前loss值为"</span>)</span><br><span class="line">        print(loss_end)</span><br><span class="line">        print(p_F)</span><br><span class="line"></span><br><span class="line"><span class="comment">#截取输出片段打印</span></span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">165.06777280622663</span></span><br><span class="line">[<span class="number">8.38287281</span> <span class="number">5.70538503</span> <span class="number">7.00228248</span> <span class="number">5.84052431</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">133.11940656598998</span></span><br><span class="line">[<span class="number">7.56299701</span> <span class="number">5.15781875</span> <span class="number">6.33931068</span> <span class="number">5.28536964</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">109.77775387801975</span></span><br><span class="line">[<span class="number">6.89894847</span> <span class="number">4.7147705</span>  <span class="number">5.8028157</span>  <span class="number">4.83622736</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">92.14372363473903</span></span><br><span class="line">[<span class="number">6.34836332</span> <span class="number">4.34779557</span> <span class="number">5.35839007</span> <span class="number">4.4642465</span> ]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">78.45934876430728</span></span><br><span class="line">[<span class="number">5.88318774</span> <span class="number">4.03806177</span> <span class="number">4.98325151</span> <span class="number">4.1503256</span> ]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">67.60388807433274</span></span><br><span class="line">[<span class="number">5.48405883</span> <span class="number">3.77257388</span> <span class="number">4.66167746</span> <span class="number">3.88128352</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">58.833090378271265</span></span><br><span class="line">[<span class="number">5.13715314</span> <span class="number">3.54205652</span> <span class="number">4.38244476</span> <span class="number">3.64771203</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">51.6356735906825</span></span><br><span class="line">[<span class="number">4.83232077</span> <span class="number">3.3397007</span>  <span class="number">4.13731374</span> <span class="number">3.44270456</span>]</span><br><span class="line"><span class="meta">... </span>... ... ... ... ... ... ... ... ... ... ...</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.34148441938663887</span></span><br><span class="line">[<span class="number">0.38019982</span> <span class="number">0.42063884</span> <span class="number">0.60690939</span> <span class="number">0.49356868</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.33762403308295663</span></span><br><span class="line">[<span class="number">0.37029135</span> <span class="number">0.41401346</span> <span class="number">0.59891746</span> <span class="number">0.48685882</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3343170568629769</span></span><br><span class="line">[<span class="number">0.36058041</span> <span class="number">0.40751086</span> <span class="number">0.5910724</span>  <span class="number">0.48027121</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3315343883568477</span></span><br><span class="line">[<span class="number">0.3510625</span>  <span class="number">0.40112821</span> <span class="number">0.58337076</span> <span class="number">0.47380298</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3292483287899748</span></span><br><span class="line">[<span class="number">0.3417333</span>  <span class="number">0.39486274</span> <span class="number">0.57580922</span> <span class="number">0.46745137</span>]</span><br></pre></td></tr></tbody></table></figure></div><p>以上代码可以看出，loss值不断不断的下降，并且最终趋于稳定，可以说明由之前的推导总结出的方法思路并无问题！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列4: Logistic Regression</title>
      <link href="/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>逻辑回归，是用于处理因变量为分类变量的回归问题，比如二分类问题，其实是一种分类算法。本文主要对比逻辑回归回归与线性回归的处理过程民兵探讨其中一些公式、细节。</p><h2 id="逻辑回归-amp-线性回归"><a href="#逻辑回归-amp-线性回归" class="headerlink" title="逻辑回归&线性回归"></a>逻辑回归&线性回归</h2><a href="https://user-images.githubusercontent.com/60562661/74258370-2bb15680-4d31-11ea-9d41-08b967ee8643.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 45%;" data-src="https://user-images.githubusercontent.com/60562661/74258370-2bb15680-4d31-11ea-9d41-08b967ee8643.png" class="lazyload"></a><p>如上图所示，</p><ul><li>step1确定函数时，逻辑回归函数输出的是个概率值，线性回归输出的可以是任何一个数</li><li>step2确定损失函数，逻辑回归用的<strong>交叉熵</strong>，线性回归则是均方误差</li><li><strong>step3比较神奇，两个损失函数长得差别很大，但是算到最后更新参数公式竟然是一样的！(后面有推导)</strong></li></ul><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>背景，一系列函数设置：</p> <a href="https://user-images.githubusercontent.com/60562661/74258366-2b18c000-4d31-11ea-83e1-e5ba4074b47e.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:40%;" data-src="https://user-images.githubusercontent.com/60562661/74258366-2b18c000-4d31-11ea-83e1-e5ba4074b47e.png" class="lazyload"></a><p>设计损失函数：这里有一个假设， x1 x2 …xN 属于 C1 类，真值为1，其余为0</p><p><a href="https://user-images.githubusercontent.com/60562661/74258376-2d7b1a00-4d31-11ea-96ba-0cc3210328f7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74258376-2d7b1a00-4d31-11ea-96ba-0cc3210328f7.png" class="lazyload"></a></p><p><strong>这里的之所以要取对数，是因为拆开之后求微分比较容易。</strong>蓝色线画的，即是交叉熵，也就是表格中step2的C函数。</p><a href="https://user-images.githubusercontent.com/60562661/74258361-26540c00-4d31-11ea-8693-ba5cdf8dbc64.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:45%;" data-src="https://user-images.githubusercontent.com/60562661/74258361-26540c00-4d31-11ea-8693-ba5cdf8dbc64.png" class="lazyload"></a><p>上述函数对w求偏微分之后，也就是求出了梯度，（过程可以自己算一下，挺简单的），就得到了下面的公式，此时更新参数就和线性回归想同也推出来了。</p><h2 id="交叉熵和均方误差"><a href="#交叉熵和均方误差" class="headerlink" title="交叉熵和均方误差"></a>交叉熵和均方误差</h2><p><a href="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" class="lazyload"></a></p><p>图中黑色曲面的代表<strong>交叉熵</strong>，红色的曲面代表<strong>均方误差</strong>，这幅图表达了参数的变化对于整个loss值的影响大小</p><ul><li>对于<strong>Cross Entropy</strong>，越边缘的点梯度越大，此时更新参数会比较快</li><li>对于 <strong>Square Error</strong>，边缘的点梯度并不大，而接近最小值的点梯度也比较小</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>由上面的对比可知：逻辑回归如果用均方误差，会导致更新参数非常慢，而直接提高学习率也并不是一个好的选择，因为真正靠近真值时，梯度本来就比较小，较大的学习率此时也并不合理，因此一般来说，逻辑回归(也就是分类问题)可以采用交叉熵作为损失函数。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二维数组中的查找</title>
      <link href="/2020/02/11/%E7%89%9B%E5%AE%A2%E7%BD%91%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-%E5%89%91%E6%8C%87offer%E7%B3%BB/"/>
      <url>/2020/02/11/%E7%89%9B%E5%AE%A2%E7%BD%91%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-%E5%89%91%E6%8C%87offer%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>自己最近正好有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。</p><p>最近正好也是有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。</p><h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><p><code>在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</code></p><h2 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h2><h4 id="思路一："><a href="#思路一：" class="headerlink" title="思路一："></a>思路一：</h4><p>因为数组的有序性，从左到右从上到下递增，利用这一个规律，选取左下角的数为基点，如果目标数大于该数，那么目标肯定在基点的右边，所以 *<em>column + 1 *</em>; 如果目标小于基点数，那么 *<em>row - 1 *</em>。</p><p>逻辑是这样，每移动一次，都会以目前所在的点为基点，因此会排除一行或者一列，这种复杂度就会比较低了。‘</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># array 二维列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Find</span><span class="params">(self, target, array)</span>:</span></span><br><span class="line">        hang = len(array)</span><br><span class="line">        lie = len(array[<span class="number">0</span>])</span><br><span class="line">        i = hang - <span class="number">1</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        result = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span>(i>=<span class="number">0</span> <span class="keyword">and</span> j<lie):< span><br><span class="line">            <span class="keyword">if</span> target < array[i][j]:</span><br><span class="line">                i = i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target > array[i][j]:</span><br><span class="line">                j = j + <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target == array[i][j]:</span><br><span class="line">                result = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> result</span><br></lie):<></span></pre></td></tr></tbody></table></figure></div><h4 id="思路二："><a href="#思路二：" class="headerlink" title="思路二："></a>思路二：</h4><p>遍历每一行用二分查找法，这个代码还没写，所以明天补上，这种方法比较通俗易懂。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 算法题目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列3: 模型的误差来源及其改进</title>
      <link href="/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/"/>
      <url>/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>之前文章讲到过用梯度下降法来更新参数。深度学习反向传播，根据误差调整参数，那么深度学习的<code>error</code>到底来自哪里？这个是比较必要的，可以给我们提高自己的模型提供方案。</p><p>这个李宏毅老师讲的很清楚，有需要可以学习一下，附上链接:<a href="https://www.bilibili.com/video/av48285039?p=8" target="_blank" rel="noopener">https://www.bilibili.com/video/av48285039?p=8</a></p><h2 id="Error来源概述"><a href="#Error来源概述" class="headerlink" title="Error来源概述"></a>Error来源概述</h2><p>深度学习种<code>Error</code> 来源于两方面：</p><ul><li><strong>bias</strong> 偏置</li><li><strong>variance</strong> 方差</li></ul><a href="https://user-images.githubusercontent.com/60562661/74254221-1cc7a580-4d2b-11ea-9b91-35b7d92a3efc.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74254221-1cc7a580-4d2b-11ea-9b91-35b7d92a3efc.png" class="lazyload"></a><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>直观上，方差可以了解为散布的分散程度。上图可以明显的看出比较高的方差，点的散布很分散，比较低的方差则比较紧凑。方差可以用来形容模型的稳定性。一般来说，</p><p>比较<strong>简单</strong>的模型，方差较<strong>小</strong>，分布比较<strong>紧密</strong>；比较<strong>复杂</strong>的模型，方差比较<strong>大</strong>，分布比较<strong>分散</strong></p><p>可以如下解释：</p><ul><li>比较简单的模型，比如 f(x) = c，受数据的影响为0，因此所有值相同，方差为0，分布都在一起</li><li>比价复杂的模型，比如五次方方程，受到数据的影响会比较大，取到的值很多，因此每次的预测值可能相差很远，此时方差就比较大了</li></ul><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>将所有预测出来的函数求出期望值得到的函数距离真实函数依然会有一段距离，这就是<strong>bias</strong>，如下图所示，</p><p><code>红线表示100个预测出来的函数，蓝线表示这100个函数的平均，黑线表示真实函数</code></p><a href="https://user-images.githubusercontent.com/60562661/74254439-6912e580-4d2b-11ea-881f-9cfd6cbbafc9.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 50%;" data-src="https://user-images.githubusercontent.com/60562661/74254439-6912e580-4d2b-11ea-881f-9cfd6cbbafc9.png" class="lazyload"></a><p>可以看出，<strong>简单</strong>的模型bias比较<strong>大</strong>，而<strong>复杂</strong>的模型bias比较<strong>小</strong></p><p>可以解释为，上图的底部，<strong>我们所设计模型的复杂程度其实也就决定了函数所能表达的范围</strong>，过于简单的函数所包含的范围可能根本没有包含到真值，因此bias比较大；而比较复杂的函数范围比较大，包含到了真值，因此bias会比较小。</p><h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><table><thead><tr><th>模型复杂程度</th><th>方差 Variance(精确性)</th><th>偏置Bias(准确性)</th></tr></thead><tbody><tr><td>Simple Model</td><td>较小</td><td>较大</td></tr><tr><td>Complex Model</td><td>较大</td><td>较小</td></tr></tbody></table><h2 id="总结与改进"><a href="#总结与改进" class="headerlink" title="总结与改进"></a>总结与改进</h2><p>了解了误差的来源，那么如何判断自己的模型是什么问题呢？</p><ul><li><p>如果模型无法适应训练数据，也就是说在<strong>训练集上误差比较大</strong>，此时就是 <strong>bias</strong> 比较大，此时处于<strong>欠拟合状态，（underfitting）</strong>也就是模型过于简单，此时改进：</p><ul><li>载入更多的特征</li><li>创建更复杂的模型</li><li><strong>注意，此时收集数据集增加数据集并起不到作用</strong></li></ul></li><li><p>如果模型在训练集表现得很好，但是在测试集表现得很差，此时就是<strong>Variance比较大</strong>，也就是<strong>过拟合（overfitting）</strong>了，此时改进：</p><ul><li>增加数据集，对于过拟合是一个很好的解决办法，如果采集不到更多的数据，则可以利用现有的数据去生成更多新的数据</li><li><strong>Regularization</strong> 正则化 ，正则化的效果是使得到的曲线更加的平滑，如下图</li></ul></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74254459-6dd79980-4d2b-11ea-950a-df5101cc68af.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74254459-6dd79980-4d2b-11ea-950a-df5101cc68af.png" class="lazyload"></a></p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>以线性回归为例，在损失函数的最后加上一项 参数 <strong>wi的平方和</strong>，这就会要求w越小越好，起到限制作用</p><p><a href="https://user-images.githubusercontent.com/60562661/74254484-77f99800-4d2b-11ea-8a9b-d9a6e32b63cf.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74254484-77f99800-4d2b-11ea-8a9b-d9a6e32b63cf.png" class="lazyload"></a></p><ul><li><p>此时函数会比较平滑，平滑即对输入不会非常敏感，至于为什么会比较平滑，可以看上图下面部分，当xi 变化时，输出结果就加了一项 wi * 变化量， 若wi 很小接近0，对结果影响并不大，因此会比较平滑。</p></li><li><p>“莱姆大” 正则化项的系数控制了函数的平滑程度，函数不是月平滑越好，有个度，因此有一个系数控制</p></li><li><p>对于偏置b，不用加正则化项是因为 ： 正则化使得函数更加平滑，能过滤掉杂色信息，而偏置 b 只能使得函数上下移动，对本来的目的并没有帮助。因此正则化项没有偏置 b 。</p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列2: 梯度下降法 ②</title>
      <link href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/"/>
      <url>/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h1 id="梯度下降法及其优化"><a href="#梯度下降法及其优化" class="headerlink" title="梯度下降法及其优化"></a>梯度下降法及其优化</h1><p>在上一篇文章中已经讲述了梯度下降法的基本流程，也讲了梯度下降法存在的问题，这篇文章就来继续讲解梯度下降法的后续。</p><p>前一篇文章提到学习率的问题，如果学习率过大会导致震荡而难以收敛，过小的话又会收敛太慢，耗费时间，因此出现了Adagrad.</p><h2 id="Tip1：Adaptive-Learning-Rate"><a href="#Tip1：Adaptive-Learning-Rate" class="headerlink" title="Tip1：Adaptive Learning Rate"></a>Tip1：Adaptive Learning Rate</h2><p>学习率对于参数调整影响很大，因此需要仔细地调整，手动调节参数肯定是不太现实，而自适应的调整参数有以下两个原则：</p><ul><li>开始阶段学习率比较大</li><li>随着迭代次数的增加越来越小</li></ul><h2 id="Adagrad-Gradient-Descent"><a href="#Adagrad-Gradient-Descent" class="headerlink" title="Adagrad Gradient Descent"></a>Adagrad Gradient Descent</h2><h3 id="与Gradient-Descent的比较"><a href="#与Gradient-Descent的比较" class="headerlink" title="与Gradient Descent的比较"></a>与Gradient Descent的比较</h3><p><a href="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" class="lazyload"></a></p><h3 id="数学公式推导"><a href="#数学公式推导" class="headerlink" title="数学公式推导"></a>数学公式推导</h3><p>原始学习率变成了 <code>常数/梯度的平方和开根号</code>，这个公式也是又推导过程的，如下(笔记字比较丑)：</p><p><a href="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" class="lazyload"></a></p><h3 id="其中的矛盾"><a href="#其中的矛盾" class="headerlink" title="其中的矛盾"></a>其中的矛盾</h3><p>观察最后一步，可以看出后面的  *<em>梯度 *</em> 和  *<em>分母的梯度的平方和开根号 *</em> （最后一行红笔画出来的）是矛盾的，梯度越大，分母也会越大，约束整个函数，但是效果会比较好。这个可以解释为，不只是考虑了一阶偏导数g(t),同时也考虑了二阶偏导数，所以结果会更加准确。(具体的可以参考李宏毅深度学习视频，前一篇文章有提到。)</p><h3 id="Adagrad总结"><a href="#Adagrad总结" class="headerlink" title="Adagrad总结"></a>Adagrad总结</h3><p>Adagrad梯度下降法其实是实现了自适应的去调节学习率，不在需要手动的仔细去调节。</p><h2 id="Tip2：Stochastic-Gradient-Descent"><a href="#Tip2：Stochastic-Gradient-Descent" class="headerlink" title="Tip2：Stochastic Gradient Descent"></a>Tip2：Stochastic Gradient Descent</h2><p>随机梯度下降法也是比较常见的，是梯度下降法的一种变体，改变也不多，它与梯度下降法的不同之处在于：</p><p>​        假设有十组训练数据，也就是十个函数，梯度下降法会把每一组参数带进去计算损失求和，每组参数的梯度也相应会进行求和嘛，然后才更新一次参数；随机梯度下降法不再需要求和这一过程，即随便一组数据带入损失函数求出梯度直接更新参数。</p><p>这种方法听起来也比较一般，它的主要优点在于<strong>更新参数快，</strong>  天下武功，唯快不破嘛。</p><h2 id="Tip3：Feature-scaling"><a href="#Tip3：Feature-scaling" class="headerlink" title="Tip3：Feature scaling"></a>Tip3：Feature scaling</h2><p>特征归一化。在做梯度下降法时，只要梯度分不一样，可以归一化之后在做，主要是因为如果某一个特征值非常大，那么它所占的比重就会非常大，这对学习非常不利。</p><p>常用的一个归一化方法：</p><p>对于特征 x1,x2,x3······xn,每种特征的某一个维度求一下这列数的均值和标准差，然后原始数据减去均值除以标准差即可归一化，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" class="lazyload"></a></p><p><strong>这样做之后这一维数据就会服从均值为0，方差为1的高斯分布。</strong></p><h2 id="Code-梯度下降法python实现"><a href="#Code-梯度下降法python实现" class="headerlink" title="Code:梯度下降法python实现"></a>Code:梯度下降法python实现</h2><h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h3><p><strong>函数（model）</strong>：y = w1* x^2 + w2*x + b ,计算出参数 w1 w2 b，给顶的真值是w1_truth = 1.8，w2_truth=2.4，b_truth = 5.6</p><p><strong>loss function</strong>   ：均方误差，公式写出太麻烦，就是y的真实值减去y的预测值的平方和</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写一个深度学习的回归案例</span></span><br><span class="line"><span class="comment">#y = w1*x^2 + w2*x + b ,计算出参数 w1 w2 b</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据 </span></span><br><span class="line">x_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line">w1_truth = <span class="number">1.8</span></span><br><span class="line">w2_truth = <span class="number">2.4</span></span><br><span class="line">b_truth = <span class="number">5.6</span></span><br><span class="line">y_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y_data[i] = w1_truth*x_data[i]*x_data[i] + w2_truth*x_data[i] + b_truth</span><br><span class="line">print(y_data.shape)</span><br><span class="line">print(x_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">w1 = <span class="number">6</span></span><br><span class="line">w2 = <span class="number">4</span></span><br><span class="line">b = <span class="number">8</span></span><br><span class="line">lr = <span class="number">1</span></span><br><span class="line">steps = <span class="number">100000</span></span><br><span class="line">lr_w1 = <span class="number">0</span></span><br><span class="line">lr_w2 = <span class="number">0</span></span><br><span class="line">lr_b = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(steps):</span><br><span class="line">    w1_grad = <span class="number">0</span></span><br><span class="line">    w2_grad = <span class="number">0</span></span><br><span class="line">    b_grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        w1_grad = w1_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* (x_data[i]**<span class="number">2</span>)</span><br><span class="line">        w2_grad = w2_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* x_data[i]</span><br><span class="line">        b_grad = b_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* <span class="number">1.0</span></span><br><span class="line">    lr_w1 = lr_w1 + w1_grad**<span class="number">2</span></span><br><span class="line">    lr_w2 = lr_w2 + w2_grad**<span class="number">2</span></span><br><span class="line">    lr_b = lr_b + b_grad**<span class="number">2</span></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    w1 = w1 - lr/np.sqrt(lr_w1) * w1_grad</span><br><span class="line">    w2 = w2 - lr/np.sqrt(lr_w2) * w2_grad</span><br><span class="line">    b  = b  - lr/np.sqrt(lr_b) * b_grad</span><br><span class="line">print(<span class="string">"w1= %f,w2 = %f, b= %f"</span> % (w1,w2,b))</span><br><span class="line"><span class="comment">#以下是代码的输出结果，可以自己测试一下，完美找到了真实值</span></span><br><span class="line"><span class="comment"># w1= 1.800000,w2 = 2.400000, b= 5.600000</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="代码中的一些细节"><a href="#代码中的一些细节" class="headerlink" title="代码中的一些细节"></a><strong>代码中的一些细节</strong></h3><ul><li><p>以上代码如果只用一个学习率，收敛会很慢，而且10万个迭代也不会迭代导最终结果，差距还是比较大的，因此代码用的是Adagrad梯度下降法，纯粹手工实现的，可以完美的预测出结果</p></li><li><p>至于代码中的计算 w1 w2 b 的梯度公式，可以手工推导出来，如下图：</p></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门系列1: 梯度下降法①</title>
      <link href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
      <url>/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>我是研究CV方向的，但是深度学习我只是在学习Tensorflow书的时候学过里面的东西，很多东西讲的过于简单，理解并不直观，所以最近就在学习深度学习的视频，是李宏毅老师讲的，内容很生动，解决了我很多疑惑，附上视频地址->，看视频就要跟着老师推导一遍公式，也是理解了很多内在的东西。</p><p><strong>bilibili</strong> : <a href="https://www.bilibili.com/video/av48285039?p=43" target="_blank" rel="noopener">https://www.bilibili.com/video/av48285039?p=43</a></p><p><strong>Youtube</strong>: <a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ" target="_blank" rel="noopener">https://www.youtube.com/watch?v=D_S6y0Jm6dQ</a></p><h2 id="概述：深度学习流程"><a href="#概述：深度学习流程" class="headerlink" title="概述：深度学习流程"></a>概述：深度学习流程</h2><p>深度学习过程于机器学习类似，这里就先讲一下机器学习的，深度学习后面会讲到。对于一个特定的<strong>机器学习</strong> <code>Machine Learning</code>任务，简单来说有以下固定的三个步骤：</p><ul><li><p><strong>定义</strong>一系列的函数 ；</p></li><li><p>评价函数的好坏，也就是定义损失函数；</p></li><li><p>找出最好的函数(model)</p></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74164678-14099d80-4c5f-11ea-961c-fd913422405a.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164678-14099d80-4c5f-11ea-961c-fd913422405a.png" class="lazyload"></a></p><p>在定义好函数和确定损失函数之后，就要开始调整参数。这时候就需要用到<strong>梯度下降法</strong>了。</p><h2 id="梯度下降法概述"><a href="#梯度下降法概述" class="headerlink" title="梯度下降法概述"></a>梯度下降法概述</h2><p>调整参数就是为了师让损失函数最小，也就是选择出最好的函数 （Model）。先讲一下梯度下降法更新参数的真个流程。</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>首先，梯度Gradient，就是在这一点的微分（偏导数），也就是曲线在该点切线的斜率，高等数学就有讲过；不理解微分就可以当作是等高线图的法线方向。梯度代表了函数上升最快的方向，所以一个函数在一点求出梯度，然后按照梯度的反方向偏移就是减小最快的。</p><p><strong>所以这里也可以得知，首先需要一个可微的函数。</strong></p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><a href="https://user-images.githubusercontent.com/60562661/74164735-208df600-4c5f-11ea-842d-dd446164c461.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164735-208df600-4c5f-11ea-842d-dd446164c461.png" class="lazyload"></a></p><ol><li><p>假设对于参数w，要更新w，首先要初始化w的值，可以随机初始化一个值，当然也有其他方法这里并不关心</p></li><li><p>计算损失函数 Loss function，L(·)对于参数w在该点处的偏导数，这就是函数在这一点的梯度。为什么是偏导数，因为参数不只是w一个，也有bias偏置b。</p></li><li><p>计算出梯度就知道在那个方向上损失函数降幅最大，就开始更新参数，直到一个局部最小值。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_new = w_original - lr*w_original_graident</span><br></pre></td></tr></tbody></table></figure></div></li><li><p>重复 <strong>2、3</strong> 步骤，就可以不断调整参数</p></li></ol><p><strong>Note :</strong> </p><ul><li>上述伪代码中有一个 lr参数，这是学习率Learning Rate，学习率决定了每次下降的步长，学习率影响还是比较大的，过小需要的时间太久，过大的话又会震荡不能收敛，这也是梯度下降法的一个存在的问题，后续会继续讲。</li><li>上述第三步提到会函数会降低到局部最小值，这里整个是以线性函数为例，故不用考虑是否会降到局部最小值。</li></ul><hr><h3 id="关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。"><a href="#关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。" class="headerlink" title="关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。"></a>关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。</h3></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo_blog搭建2-主题相关</title>
      <link href="/2020/02/09/hexo-blog%E6%90%AD%E5%BB%BA2-%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3/"/>
      <url>/2020/02/09/hexo-blog%E6%90%AD%E5%BB%BA2-%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>我的这个版本博客是基于Hexo搭建的，主题采用的是 <a href="https://jerryc.me/posts/21cfbf15/#" target="_blank" rel="noopener">Butterfly</a>，在这个网址有详细的安装配置教程。 这个主题我非非常喜欢，看了很多的主题就相中了这款。然而配置也是花了我整整一天，可能还算是比较顺利把！配置过程中有的地方安装文档讲的不是很详细，我自己也踩坑了，来记录一下！</p><p> <a href="https://user-images.githubusercontent.com/60562661/74105856-93826880-4b9c-11ea-8629-4b1b724d54f4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105856-93826880-4b9c-11ea-8629-4b1b724d54f4.png" class="lazyload"></a></p><h2 id="添加-Gallery（相册）"><a href="#添加-Gallery（相册）" class="headerlink" title="添加 Gallery（相册）"></a>添加 Gallery（相册）</h2><h3 id="新建页面"><a href="#新建页面" class="headerlink" title="新建页面"></a>新建页面</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page gallery <span class="comment">#此时会在source目录下产生gallery/index.md</span></span><br></pre></td></tr></tbody></table></figure></div><p>打开来编辑这个md文件，有两点要注意的：</p><ul><li>开头的 <strong>type: “gallery”</strong>   不能出错</li><li>内容格式 按照  <strong><div class="justified-gallery"><p>img1  img2  img3 </p>          </div></strong>    来写</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74105571-12c26d00-4b9a-11ea-8e58-9cb4520896fd.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105571-12c26d00-4b9a-11ea-8e58-9cb4520896fd.png" class="lazyload"></a></p><h3 id="添加导航栏"><a href="#添加导航栏" class="headerlink" title="添加导航栏"></a>添加导航栏</h3><p>在 butterfly.yml 文件的menu下照着之前的添加相册，如下图</p><p><a href="https://user-images.githubusercontent.com/60562661/74105539-d5f67600-4b99-11ea-83f1-f856a034b9b3.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105539-d5f67600-4b99-11ea-83f1-f856a034b9b3.png" class="lazyload"></a></p><p>至此，相册添加结束</p><h2 id="添加本地搜索"><a href="#添加本地搜索" class="headerlink" title="添加本地搜索"></a>添加本地搜索</h2><h3 id="安装hexo本地搜索的插件"><a href="#安装hexo本地搜索的插件" class="headerlink" title="安装hexo本地搜索的插件"></a>安装hexo本地搜索的插件</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></tbody></table></figure></div><h3 id="配置全局的config文件"><a href="#配置全局的config文件" class="headerlink" title="配置全局的config文件"></a>配置全局的config文件</h3><p><a href="https://user-images.githubusercontent.com/60562661/74105544-da229380-4b99-11ea-8ba8-7715f23704b9.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105544-da229380-4b99-11ea-8ba8-7715f23704b9.png" class="lazyload"></a></p><h3 id="配置butterfly文件"><a href="#配置butterfly文件" class="headerlink" title="配置butterfly文件"></a>配置butterfly文件</h3><p>这里的配置和官方教程一样，不再赘述</p><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><p>我自己在这里一直配置不好，刚开始发现是找不到 search.xml 文件，后来我在博客插件目录找到了，然后改上图的path，这样在本地启动blog是可以搜索的，但是推送到远程就不行了，后面我重装了插件，path改成默认路径,然后 <code>hexo clean</code>  <code>hexo g</code> <code>hexo d</code>  就很神奇的在github仓库生成了search.xml文件，也就随之可以用了，具体原理我还没搞明白，以后明白了再更新。</p><h2 id="添加评论"><a href="#添加评论" class="headerlink" title="添加评论"></a>添加评论</h2><p>我用的是 Valine 评论系统，这个很容易搭建起来，大致描述一下流程：</p><h3 id="配置-valine"><a href="#配置-valine" class="headerlink" title="配置 valine"></a>配置 valine</h3><ol><li><p>注册 <a href="https://www.leancloud.cn/" target="_blank" rel="noopener">https://www.leancloud.cn/</a> 在这个网站注册账号，然后进行实名认证</p></li><li><p>创建一个应用</p></li><li><p>进入设置页面，在应用keys中可以看到自己的 <strong>AppID</strong> <strong>AppKey</strong>   等会儿会用到</p></li><li><p><strong>在安全中心页面 web安全域名写入自己的博客域名</strong>（这一步很重要，我就是一开始在这里栽了）</p><p><strong>至此，valine配置结束</strong></p></li></ol><h3 id="配置主题文件"><a href="#配置主题文件" class="headerlink" title="配置主题文件"></a>配置主题文件</h3><p>这里的流程和官方教程一样，很容易，自己直接会明白</p><p><strong>我在这里卡住是因为写 APPID APPKey出了问题，卡了很久才发现</strong></p><p>然后重新更新生成就有了评论功能！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 踩坑合集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> 博客主题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拥有自己优雅的图床</title>
      <link href="/2020/02/09/%E6%8B%A5%E6%9C%89%E8%87%AA%E5%B7%B1%E4%BC%98%E9%9B%85%E7%9A%84%E5%9B%BE%E5%BA%8A/"/>
      <url>/2020/02/09/%E6%8B%A5%E6%9C%89%E8%87%AA%E5%B7%B1%E4%BC%98%E9%9B%85%E7%9A%84%E5%9B%BE%E5%BA%8A/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p> 图床，就是专门用来存放图片的空间，不过与本地不同，图床是存储在网络上的。这样图片会有个地址，可以访问到图片，也可以引用。如果写个人博客等等肯定用得上。github这个天然的图床不用就太可惜了！ 所以选择github来作为自己图床。</p><p><a href="https://user-images.githubusercontent.com/60562661/74098587-0831b480-4b55-11ea-992f-e7dd9181abe7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098587-0831b480-4b55-11ea-992f-e7dd9181abe7.png" class="lazyload"></a></p><h2 id="仓库准备"><a href="#仓库准备" class="headerlink" title="仓库准备"></a>仓库准备</h2><p>可以新建一个仓库，也可以在现有的仓库下。具体怎么建仓库请自己寻找。</p><p><strong>下面以我自己的仓库为例</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/74098589-09fb7800-4b55-11ea-9ac8-6995b63f32e4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098589-09fb7800-4b55-11ea-9ac8-6995b63f32e4.png" class="lazyload"></a></p><h2 id="进入issues新建问题"><a href="#进入issues新建问题" class="headerlink" title="进入issues新建问题"></a>进入issues新建问题</h2><p>点击issues，然后点击右上角NewIssues，进入如下界面</p><p><a href="https://user-images.githubusercontent.com/60562661/74098590-0a940e80-4b55-11ea-9157-59279231e403.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098590-0a940e80-4b55-11ea-9157-59279231e403.png" class="lazyload"></a></p><p>关键部分红色框已经框出来，可以在这里上传自己的图片，然后点击提交即可，图下图</p><p><a href="https://user-images.githubusercontent.com/60562661/74098591-0b2ca500-4b55-11ea-8540-b66aafb27fa7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098591-0b2ca500-4b55-11ea-8540-b66aafb27fa7.png" class="lazyload"></a></p><p>此时点击图片，即可打开一个新的链接，此时图片的网址就可以被引用了。</p><p><strong>Enjoy！</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图床 </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自己的域名与github绑定</title>
      <link href="/2020/02/06/%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%E4%B8%8Egithub%E7%BB%91%E5%AE%9A/"/>
      <url>/2020/02/06/%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%E4%B8%8Egithub%E7%BB%91%E5%AE%9A/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>一般情况下用github托管的博客很方便，但是域名访问只能用 <a href="https://xxxx.github.io来访问，这时候就有与自己的域名绑定的问题，即自己的域名指向github地址。（以腾讯云为例）" target="_blank" rel="noopener">https://xxxx.github.io来访问，这时候就有与自己的域名绑定的问题，即自己的域名指向github地址。（以腾讯云为例）</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73957566-7ef66400-4941-11ea-87ee-e9c53fdbc305.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957566-7ef66400-4941-11ea-87ee-e9c53fdbc305.png" class="lazyload"></a></p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>开始操作前，确保自己已经有：</p><ul><li><strong>可用</strong>的域名：注意不需要备案（我的是这样），但是大多数需要实名认证</li><li>有自己的<strong>github</strong>项目地址，确保可以访问</li></ul><h2 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h2><p>进入腾讯云控制台，在自己的域名解析处设置如下：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957562-7dc53700-4941-11ea-81f5-472730958808.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957562-7dc53700-4941-11ea-81f5-472730958808.png" class="lazyload"></a></p><p>这里要注意两点：</p><ul><li>记录类型选择 <strong>CNAME</strong></li><li>记录值为自己的github项目地址（<strong>github域名</strong>）</li></ul><p>此时如果解析成功，<strong>ping</strong>一下测试：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957564-7e5dcd80-4941-11ea-93c1-501060db97b2.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957564-7e5dcd80-4941-11ea-93c1-501060db97b2.png" class="lazyload"></a></p><h2 id="github仓库设置"><a href="#github仓库设置" class="headerlink" title="github仓库设置"></a>github仓库设置</h2><p>进入github项目仓库设置，往下翻在 Github Page 中自定义地址写自己的域名 然后保存，此时主页会自动生成 CNAME 文件，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957559-7c940a00-4941-11ea-8f4c-b56ac851d659.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957559-7c940a00-4941-11ea-8f4c-b56ac851d659.png" class="lazyload"></a></p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><h3 id="等待时间"><a href="#等待时间" class="headerlink" title="等待时间"></a>等待时间</h3><p>等待10-15min，就可以通过自己的域名来访问了。注意能ping通说明已经成功，此时就不要频繁修改设置了，影响同步 时间</p><h3 id="普适性"><a href="#普适性" class="headerlink" title="普适性"></a>普适性</h3><p>以上流程在hugo博客上毫无问题，原因是hugo博客推送到服务器是推送public中的内容，而CNAME在根目录并不会影响；</p><p>但是在hexo博客上就有问题了，我查了一下发现如果直接在gtihub直接指定，会在github仓库上直接生成CNAME文件，而这时候有个问题就是在写文章同步的话，本地并没有CNAME文件，会直接覆盖掉已经生成的CNAME 文件，此时指定的域名就不生效了。因此有了以下的章节。</p><h2 id="Hexo博客绑定域名"><a href="#Hexo博客绑定域名" class="headerlink" title="Hexo博客绑定域名"></a>Hexo博客绑定域名</h2><p>首先在本地 博客根目录中source文件夹下新建一个CNAME文件，内容就是自己的域名，然后再按照上面去github指定。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 域名 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0开始搭建私人云盘</title>
      <link href="/2020/02/06/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E7%A7%81%E4%BA%BA%E4%BA%91%E7%9B%98/"/>
      <url>/2020/02/06/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E7%A7%81%E4%BA%BA%E4%BA%91%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>现在的云盘有很多，百度云、蓝奏云、腾讯微云等等，看起来似乎是没有搭建云盘的必要，但是百度云限速让我觉得恶心，不开超级会员就非常非常非常慢，开了又觉得亏，所以就想到了能不能自己搭建一个云盘。自己搭建的私有云还有一个特点就是比较安全，其实只是自己比较喜欢乱鼓捣新东西。现在的下载速度并不快，我还没研究，但是上传速度经过修改可以达到2M/s以上，其实我的云服务器比较垃圾，毕竟是1块钱白嫖了一个月–。</p><p><strong>先上链接：</strong></p><p><a href="http://111.229.74.50/kodexplorer/index.php?explorer" target="_blank" rel="noopener">http://111.229.74.50/kodexplorer/index.php?explorer</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73954603-02618680-493d-11ea-9ebd-b33ee3002b8e.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954603-02618680-493d-11ea-9ebd-b33ee3002b8e.png" class="lazyload"></a></p><h2 id="购买一个云服务器"><a href="#购买一个云服务器" class="headerlink" title="购买一个云服务器"></a>购买一个云服务器</h2><p>云服务器性能越好，与上传下载速度都有很大关系，结合自己个人需求、经济状况适当购买。当然也可以先白嫖一个用用体验一下。购买云服务器国内比较火的也就几个：<strong>阿里云，腾讯云，京东云，华为云。</strong>我用的腾讯云的服务器。<strong>↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/73954616-068da400-493d-11ea-9634-e7cad5cde6eb.png" data-fancybox="group" data-caption="photo" class="fancybox"><img alt="photo" title="photo" data-src="https://user-images.githubusercontent.com/60562661/73954616-068da400-493d-11ea-9634-e7cad5cde6eb.png" class="lazyload"></a></p><h2 id="开始配置服务器"><a href="#开始配置服务器" class="headerlink" title="开始配置服务器"></a>开始配置服务器</h2><p><strong>Note：服务器以windows操作系统为例；本私有云基于可道云kodexplorer</strong></p><h3 id="远程登陆到服务器"><a href="#远程登陆到服务器" class="headerlink" title="远程登陆到服务器"></a>远程登陆到服务器</h3><p>以腾讯云为例，上图中主机后面就有登录，可以直接登录，登录之后界面是这样的：(每个人的服务器界面会有点不一样，win比较方便，但是 linux更适合作为服务器系统)</p><p><a href="https://user-images.githubusercontent.com/60562661/73954608-04c3e080-493d-11ea-8fcb-52f59f4acc8d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954608-04c3e080-493d-11ea-8fcb-52f59f4acc8d.png" class="lazyload"></a></p><h3 id="配置PHP环境"><a href="#配置PHP环境" class="headerlink" title="配置PHP环境"></a>配置PHP环境</h3><p>下载安装Xampp (XAMPP 是一个把 Apache 网页服务器与 PHP、Perl 及 MariaDB 集合在一起的安装包，允许用户可以在自己的电脑上轻易的建立网页服务器环境。)</p><p>中文官网下载地址 ： <a href="https://www.apachefriends.org/zh_cn/download.html。" target="_blank" rel="noopener">https://www.apachefriends.org/zh_cn/download.html。</a> </p><p>安装下一步下一步就可以，安装完之后开启<strong>Apache、 MySQL</strong>两项，如下图</p><p><a href="https://user-images.githubusercontent.com/60562661/73954610-055c7700-493d-11ea-879d-3a9c99c2e006.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954610-055c7700-493d-11ea-879d-3a9c99c2e006.png" class="lazyload"></a></p><p>点击Admin进入相应页面（如下图）即说明已经配置好。就是这么容易 。</p><p><a href="https://user-images.githubusercontent.com/60562661/73954613-05f50d80-493d-11ea-86f3-da2ae7e8a8b0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954613-05f50d80-493d-11ea-86f3-da2ae7e8a8b0.png" class="lazyload"></a></p><h3 id="下载和安装可道云kodexplorer"><a href="#下载和安装可道云kodexplorer" class="headerlink" title="下载和安装可道云kodexplorer"></a>下载和安装可道云kodexplorer</h3><p>下载地址：<a href="http://kodcloud.com/download.html" target="_blank" rel="noopener">http://kodcloud.com/download.html</a></p><p>下载完成后解压，将可道云文件夹复制到xmapp安装文件夹下的htdocs文件夹，此时已经完成</p><p><strong>Note: 可道云文件夹命名应为</strong><code>kodexplorer</code>(有点常识都会理解)</p><h3 id="访问自己的私有云"><a href="#访问自己的私有云" class="headerlink" title="访问自己的私有云"></a>访问自己的私有云</h3><p><a href="http://112.xxx.xx.xx/kodexplorer/index.php?user/login" target="_blank" rel="noopener">http://112.xxx.xx.xx/kodexplorer/index.php?user/login</a></p><h2 id="可道云配置优化"><a href="#可道云配置优化" class="headerlink" title="可道云配置优化"></a>可道云配置优化</h2><p>操作完2已经可以访问上传下载添加用户了。但是此时上传速度很慢，此时就需要配置一下了，以下操作我亲自测试有用，但是下载还是比较慢，我再研究一下。</p><h3 id="修改php-ini上传限制"><a href="#修改php-ini上传限制" class="headerlink" title="修改php.ini上传限制"></a>修改php.ini上传限制</h3><p>在xmapp安装目录直接搜索该文件即可</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">php</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight php"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">max_execution_time = <span class="number">3600</span></span><br><span class="line">max_input_time = <span class="number">3600</span></span><br><span class="line">post_max_size = <span class="number">150</span>M</span><br><span class="line">upload_max_filesize = <span class="number">150</span>M</span><br></pre></td></tr></tbody></table></figure></div><h3 id="修改可道云配置"><a href="#修改可道云配置" class="headerlink" title="修改可道云配置"></a>修改可道云配置</h3><p>在config/下新建 setting_user.php文件;粘贴如下内容；(已存在则略过)</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">php</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight php"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><!--?php</span--></span><br><span class="line"></span><br><span class="line"><span class="comment">//分片上传: 每个切片5M,需要php.ini 中upload_max_filesize大于此值</span></span><br><span class="line">$GLOBALS[<span class="string">'config'</span>][<span class="string">'settings'</span>][<span class="string">'updloadChunkSize'</span>] = <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>;   </span><br><span class="line"></span><br><span class="line"><span class="comment">//上传并发数量; 推荐15个并发;</span></span><br><span class="line">$GLOBALS[<span class="string">'config'</span>][<span class="string">'settings'</span>][<span class="string">'updloadThreads'</span>] = <span class="number">15</span>;</span><br></span></pre></td></tr></tbody></table></figure></div><hr><p>完结，撒花，下载速度慢研究出来原因再更新</p><p><a href="https://user-images.githubusercontent.com/60562661/73954934-7d2aa180-493d-11ea-989f-f16d19aca743.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954934-7d2aa180-493d-11ea-989f-f16d19aca743.jpg" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 黑科技 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo_blog搭建部署笔记</title>
      <link href="/2020/02/05/MyBlog-Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/"/>
      <url>/2020/02/05/MyBlog-Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p><strong>NodeJS</strong> + <strong>hexo</strong> + <strong>git</strong>, 这三个安装教程可以自行百度到，很多博客里也有，这里便不再赘述</p><h2 id="搭建博客"><a href="#搭建博客" class="headerlink" title="搭建博客"></a>搭建博客</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></tbody></table></figure></div><h3 id="写一篇新博客"><a href="#写一篇新博客" class="headerlink" title="写一篇新博客"></a>写一篇新博客</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">"helloworld"</span> <span class="comment">#此时在source/_posts下会生成文章</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></tbody></table></figure></div><p><strong>此时基本搭建完成</strong></p><h2 id="部署到github"><a href="#部署到github" class="headerlink" title="部署到github"></a>部署到github</h2><h3 id="安装deploy插件"><a href="#安装deploy插件" class="headerlink" title="安装deploy插件"></a>安装deploy插件</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-deployer-git</span><br></pre></td></tr></tbody></table></figure></div><h3 id="新建github仓库"><a href="#新建github仓库" class="headerlink" title="新建github仓库"></a>新建github仓库</h3><p>这步比较简单，可以自己查查如何新建</p><h3 id="修改全局配置文件-config-yml："><a href="#修改全局配置文件-config-yml：" class="headerlink" title="修改全局配置文件  _config.yml："></a>修改全局配置文件  _config.yml：</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">yml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/fryddup/fryddup.github.io.git</span> <span class="comment">#自己的仓库地址</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="推到远程服务器"><a href="#推到远程服务器" class="headerlink" title="推到远程服务器"></a>推到远程服务器</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -d</span><br></pre></td></tr></tbody></table></figure></div><p>此时可以通过 <a href="https://huaqi.blue" target="_blank" rel="noopener">https://huaqi.blue</a> 来访问我的博客</p><hr><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>每次写完博客 hexo clean  hexo g 两步必不可少，然后在 hexo d</p><p><a href="C:%5CUsers%5C33196%5CDesktop%5C0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="C:%5CUsers%5C33196%5CDesktop%5C0.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windwos下几款实用又美观的软件</title>
      <link href="/2020/02/02/windwos%E4%B8%8B%E5%87%A0%E6%AC%BE%E5%AE%9E%E7%94%A8%E5%8F%88%E7%BE%8E%E8%A7%82%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
      <url>/2020/02/02/windwos%E4%B8%8B%E5%87%A0%E6%AC%BE%E5%AE%9E%E7%94%A8%E5%8F%88%E7%BE%8E%E8%A7%82%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>本人是windows重症患者，经常在搜集乱七八糟（好看）的软件，因此来记录几款实用的软件。</p><h2 id="1-Typora"><a href="#1-Typora" class="headerlink" title="1. Typora"></a>1. Typora</h2><p>markdown笔记软件，大家应该都知道这个，美观又实用！</p><p>官网：<a href="https://typora.io/" target="_blank" rel="noopener">https://typora.io/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609608-62d58880-460a-11ea-8b27-655765e84224.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609608-62d58880-460a-11ea-8b27-655765e84224.gif" class="lazyload"></a></p><h2 id="2-Choaolatey"><a href="#2-Choaolatey" class="headerlink" title="2. Choaolatey"></a>2. Choaolatey</h2><p>这是一款windows下的包管理器，可以像 linux 一样安装各种包，很是方便！</p><p>官网：<a href="https://chocolatey.org/" target="_blank" rel="noopener">https://chocolatey.org/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609604-61a45b80-460a-11ea-923c-16414aa000a9.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609604-61a45b80-460a-11ea-923c-16414aa000a9.gif" class="lazyload"></a></p><h2 id="3-hyper"><a href="#3-hyper" class="headerlink" title="3. hyper"></a>3. hyper</h2><p>hyper是 windows下面很炫酷的一个终端嘛，有各种皮肤、插件！</p><p>官网：<a href="https://hyper.is/" target="_blank" rel="noopener">https://hyper.is/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609605-623cf200-460a-11ea-9ac7-f9c937f919fd.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609605-623cf200-460a-11ea-9ac7-f9c937f919fd.gif" class="lazyload"></a></p><h2 id="4-Listary"><a href="#4-Listary" class="headerlink" title="4. Listary"></a>4. Listary</h2><p>这是一款很方便搜索打开本地文件的软件，有很多功能值得探索</p><p>官网：<a href="https://www.listary.com/" target="_blank" rel="noopener">https://www.listary.com/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609607-623cf200-460a-11ea-915a-f9e90a77cfd6.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609607-623cf200-460a-11ea-915a-f9e90a77cfd6.gif" class="lazyload"></a></p><h2 id="5-IobitUninstaller"><a href="#5-IobitUninstaller" class="headerlink" title="5. IobitUninstaller"></a>5. IobitUninstaller</h2><p>windows上卸载软件用的一款软件，超好用</p><p><a href="https://user-images.githubusercontent.com/60562661/73609606-623cf200-460a-11ea-9942-0fc4b33b02ba.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609606-623cf200-460a-11ea-9942-0fc4b33b02ba.png" class="lazyload"></a></p><p>后续再有好的继续补充…..</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hugo_blog搭建部署笔记</title>
      <link href="/2020/02/02/hugo_blog%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/02/02/hugo_blog%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>之前一直用的  Hexo Blog ,奈何写出来太丑了，不习惯，部署起来也比较麻烦，就找到了新的好看的，还真的是好看是第一生产力！hugo部署过程如下。</p><p><a href="https://user-images.githubusercontent.com/60562661/73609078-10459d80-4605-11ea-93b4-d26fa0f93625.png" data-fancybox="group" data-caption="hugo" class="fancybox"><img alt="hugo" title="hugo" data-src="https://user-images.githubusercontent.com/60562661/73609078-10459d80-4605-11ea-93b4-d26fa0f93625.png" class="lazyload"></a></p><h2 id="描述版："><a href="#描述版：" class="headerlink" title="描述版："></a>描述版：</h2><h4 id="1-安装-hugo-（有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）"><a href="#1-安装-hugo-（有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）" class="headerlink" title="1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）"></a>1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）</h4><p><code>choco install hugo</code></p><h4 id="2-检测go语言是否安装成功"><a href="#2-检测go语言是否安装成功" class="headerlink" title="2.检测go语言是否安装成功"></a>2.检测go语言是否安装成功</h4><p><code>hugo version</code></p><h4 id="3-开始建站-建立一个-myblog-站点（博客的根目录）"><a href="#3-开始建站-建立一个-myblog-站点（博客的根目录）" class="headerlink" title="3.开始建站,建立一个 myblog 站点（博客的根目录）"></a>3.开始建站,建立一个 myblog 站点（博客的根目录）</h4><p><code>hugo new site myblog</code></p><h4 id="4-安装hugo博客主题-官网-https-themes-gohugo-io-每个主题有对应的方式，我以我自己的为例-此时目录在博客的根目录"><a href="#4-安装hugo博客主题-官网-https-themes-gohugo-io-每个主题有对应的方式，我以我自己的为例-此时目录在博客的根目录" class="headerlink" title="4.安装hugo博客主题 官网 https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录"></a>4.安装hugo博客主题 官网 <a href="https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录" target="_blank" rel="noopener">https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录</a></h4><p><code>git clone https://github.com/flysnow-org/maupassant-hugo themes/maupassant</code></p><h4 id="5-写一篇新博客，位于content-post"><a href="#5-写一篇新博客，位于content-post" class="headerlink" title="5.写一篇新博客，位于content/post/"></a>5.写一篇新博客，位于content/post/</h4><p><code>hugo new post/Hello,world.md</code></p><h4 id="6-启动本地调试，此时位于根目录"><a href="#6-启动本地调试，此时位于根目录" class="headerlink" title="6.启动本地调试，此时位于根目录"></a>6.启动本地调试，此时位于根目录</h4><p><code>hugo server --buildDrafts</code></p><h4 id="7-新建github仓库，这个比较容易，自己解决"><a href="#7-新建github仓库，这个比较容易，自己解决" class="headerlink" title="7.新建github仓库，这个比较容易，自己解决"></a>7.新建github仓库，这个比较容易，自己解决</h4><h4 id="8-关联到GitHub-此步骤会生成public文件夹"><a href="#8-关联到GitHub-此步骤会生成public文件夹" class="headerlink" title="8.关联到GitHub,此步骤会生成public文件夹"></a>8.关联到GitHub,此步骤会生成public文件夹</h4><p><code>hugo --themes=maupassant --baseURL="https://huaqi19.github.io/"</code></p><h4 id="9-将-public-文件夹内容推送到空的github仓库中"><a href="#9-将-public-文件夹内容推送到空的github仓库中" class="headerlink" title="9.将 public 文件夹内容推送到空的github仓库中"></a>9.将 public 文件夹内容推送到空的github仓库中</h4><p>此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件</p><h4 id="10-更新博客"><a href="#10-更新博客" class="headerlink" title="10.更新博客"></a>10.更新博客</h4><p><code>hugo -D</code></p><h4 id="至此博客搭建完成，主题配置请自行研究"><a href="#至此博客搭建完成，主题配置请自行研究" class="headerlink" title="至此博客搭建完成，主题配置请自行研究"></a>至此博客搭建完成，主题配置请自行研究</h4><h2 id="代码版："><a href="#代码版：" class="headerlink" title="代码版："></a>代码版：</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）</span></span><br><span class="line">choco install hugo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.检测go语言是否安装成功</span></span><br><span class="line">hugo version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.开始建站,建立一个 myblog 站点（博客的根目录）</span></span><br><span class="line">hugo new site myblog</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.安装hugo博客主题 官网 https://themes.gohugo.io/</span></span><br><span class="line"><span class="comment"># 每个主题有对应的方式，我以我自己的为例</span></span><br><span class="line"><span class="comment"># 此时目录在博客的根目录</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/flysnow-org/maupassant-hugo themes/maupassant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.写一篇新博客，位于content/post/</span></span><br><span class="line">hugo new post/Hello,world.md</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.启动本地调试，此时位于根目录</span></span><br><span class="line">hugo server --buildDrafts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.新建github仓库，这个比较容易，自己解决</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.关联到GitHub,此步骤会生成public文件夹</span></span><br><span class="line">hugo --themes=maupassant --baseURL=<span class="string">"https://huaqi19.github.io/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.将 public 文件夹内容推送到空的github仓库中</span></span><br><span class="line"><span class="comment"># 此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10.更新博客</span></span><br><span class="line">hugo -D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 至此博客搭建完成，主题配置请自行研究</span></span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> hugo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git常用操作整合</title>
      <link href="/2020/02/02/git%E6%8E%A8%E9%80%81%E6%96%87%E4%BB%B6/"/>
      <url>/2020/02/02/git%E6%8E%A8%E9%80%81%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><p>github远程托管文件 非常的方便。git操作种比较多的就是本地文件推送到远程，仓库等。而我一直也是没有搞明白，github本地文件远程推送是如何操作的，以至于今天卡了很久，来记录一下。</p><p><a href="https://user-images.githubusercontent.com/60562661/73609075-07ed6280-4605-11ea-9971-35fe0663a9e0.jpg" data-fancybox="group" data-caption="git" class="fancybox"><img alt="git" title="git" data-src="https://user-images.githubusercontent.com/60562661/73609075-07ed6280-4605-11ea-9971-35fe0663a9e0.jpg" class="lazyload"></a></p><h2 id="1git推送本地文件"><a href="#1git推送本地文件" class="headerlink" title="1git推送本地文件"></a>1git推送本地文件</h2><h3 id="一般流程"><a href="#一般流程" class="headerlink" title="一般流程"></a>一般流程</h3><p>*<em>Note *</em>:  默认远程仓库是空的</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.新建远程仓库 ，这个可以自己去查阅，很容易</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.把本地需要推送的文件夹设置为git仓库</span></span><br><span class="line">git init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.添加文件</span></span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.提交文件</span></span><br><span class="line">git commit -m <span class="string">'first_commit'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.添加镜像源</span></span><br><span class="line">git remote add origin https://github.com/fryddup/fryddup.github.io.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.推送文件</span></span><br><span class="line">git push -u origin master</span><br></pre></td></tr></tbody></table></figure></div><h3 id="同步问题"><a href="#同步问题" class="headerlink" title="同步问题"></a>同步问题</h3><p>以上代码即可实现本地文件推送到远程，但是注意当仓库不是空的，仓库有改动，应当如下操作：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">'first_commit'</span></span><br><span class="line"></span><br><span class="line">git pull  <span class="comment">#注意，此命令即是远程文件同步到本地。</span></span><br><span class="line"></span><br><span class="line">git push -u origin master</span><br></pre></td></tr></tbody></table></figure></div><h2 id="git-代理相关"><a href="#git-代理相关" class="headerlink" title="git 代理相关"></a>git 代理相关</h2><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.proxy <span class="string">'socks5://127.0.0.1:1080'</span> </span><br><span class="line">git config --global https.proxy <span class="string">'socks5://127.0.0.1:1080'</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="查看代理"><a href="#查看代理" class="headerlink" title="查看代理"></a>查看代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --get http.proxy</span><br><span class="line">git config --global --get https.proxy</span><br></pre></td></tr></tbody></table></figure></div><h3 id="取消代理"><a href="#取消代理" class="headerlink" title="取消代理"></a>取消代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --<span class="built_in">unset</span> http.proxy</span><br><span class="line">git config --global --<span class="built_in">unset</span> https.proxy</span><br></pre></td></tr></tbody></table></figure></div><p><strong>git其他操作、问题以后遇到了再补充</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello,World</title>
      <link href="/2020/02/02/Hello,World/"/>
      <url>/2020/02/02/Hello,World/</url>
      
        <content type="html"><![CDATA[<html><head></head><body><h2 id="Linux-Ubuntu-快捷键-amp-amp-操作指令"><a href="#Linux-Ubuntu-快捷键-amp-amp-操作指令" class="headerlink" title="Linux(Ubuntu) 快捷键&&操作指令"></a>Linux(Ubuntu) 快捷键&&操作指令</h2><h5 id="对于我这个转行计算机的人来说，现在依然是-Hello，World！-乾坤未定，你我皆是黑马，自己给自己加油。"><a href="#对于我这个转行计算机的人来说，现在依然是-Hello，World！-乾坤未定，你我皆是黑马，自己给自己加油。" class="headerlink" title="对于我这个转行计算机的人来说，现在依然是 Hello，World！ 乾坤未定，你我皆是黑马，自己给自己加油。"></a>对于我这个转行计算机的人来说，现在依然是 Hello，World！ 乾坤未定，你我皆是黑马，自己给自己加油。</h5><h2 id="gt-gt-快捷键："><a href="#gt-gt-快捷键：" class="headerlink" title=">> 快捷键："></a>>> 快捷键：</h2><p>——————Terminal终端——————————</p><p><strong>CTRL + ALT + T</strong> :   打开终端<br><strong>TAB</strong>:   自动补全命令或文件名</p><h2 id="gt-gt-操作指令："><a href="#gt-gt-操作指令：" class="headerlink" title=">> 操作指令："></a>>> 操作指令：</h2><p><strong>ifconfig -a</strong>          查看ip地址、端口等网络相关信息</p><p><strong>watch -n 1 nvidia-smi</strong>          查看显卡使用情况</p><p><strong>source activate HPEG_PT</strong>      激活虚拟环境</p><p> <strong>mkvirtualenv</strong>     创建虚拟环境</p><p><strong>htop F4 f9 9 Enter</strong>   杀死某个指定进程</p><p><strong>pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> numpy(指定的库)</strong>    pip使用清华镜像安装库，不能翻墙可提高下载速度</p><p><strong>conda create -n NEW_Nmae python=3.6</strong>      conda创建虚拟环境</p><p><strong>conda install pytorch=0.4.0 torchvision cudatoolkit=9.0 -c pytorch</strong>       安装pytorch</p><p><strong>pip install tensorflow-gpu==1.12</strong>  安装任意版本tensorflow</p><h3 id="gt-gt-Note：本篇会一直更新，linux、库安装等各种指令"><a href="#gt-gt-Note：本篇会一直更新，linux、库安装等各种指令" class="headerlink" title=">>Note：本篇会一直更新，linux、库安装等各种指令-"></a>>>Note：本篇会一直更新，linux、库安装等各种指令-</h3></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
